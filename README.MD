# Route Crawler

![Interface](images/Capture.PNG)

Route Crawler is a CLI application built with Node.js that scans a given website and displays all the routes found on it. It utilizes modern web technologies such as axios for making HTTP requests and jsdom for parsing HTML, effectively extracting and listing all hyperlinks present on a webpage.

## Features

- Scans a given website for all routes.
- Handles dynamic routes and JavaScript-driven navigation.
- Outputs the total number of routes found.
- Provides a list of all discovered routes.

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/RizkyZaki/route-crawler.git
   cd route-crawler
   ```

2. Install the required dependencies:

   ```bash
   npm i
   ```

## Usage

Run the main script:

```bash
npm start crawl
```

## Contribution

Contributions and suggestions are highly appreciated. If you would like to contribute to this project, please open an _issue_ or submit a _pull request_.

## License

Distribute under the MIT license. For more information, see `LICENSE`.

---

Â© 2024 zch. Built with love for the developer community.
